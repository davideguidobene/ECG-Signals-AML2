{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f1eb86",
   "metadata": {},
   "source": [
    "## Task 2: Multi-class classification of ECG signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feacea4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import neurokit2 as nk\n",
    "import warnings\n",
    "import biosppy.signals.ecg as ecg\n",
    "import biosppy\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy.ma as ma\n",
    "from numpy import ptp, zeros, mean\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics import tsaplots\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, kurtosis\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import neurokit2 as nk\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics import tsaplots\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, kurtosis\n",
    "import biosignalsnotebooks as bsnb\n",
    "from math import log10\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics import tsaplots\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, kurtosis, pearsonr\n",
    "from scipy.signal import argrelextrema, find_peaks,correlate\n",
    "import heartpy as hp\n",
    "\n",
    "import os.path\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from IPython.display import Audio\n",
    "sound_file = 'notification.mp3'\n",
    "import pygame\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133534a4",
   "metadata": {},
   "source": [
    "### Functions preprocessing and features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d877d-0814-4747-b679-e1dafbc07832",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadData(f):\n",
    "    A = pd.read_csv(f).drop(columns = ['id'])\n",
    "    return A\n",
    "\n",
    "def processSignal(data):\n",
    "    #input: orignal matrix data\n",
    "    \n",
    "    \n",
    "    data_filtered = pd.DataFrame()\n",
    "\n",
    "    for (i, sample) in enumerate(tqdm(data.iterrows(), total=len(data))):\n",
    "        \n",
    "        ECG = sample[1].dropna().to_numpy(dtype='float32')\n",
    "        cleansed = nk.ecg_clean(ECG, sampling_rate=300, method='neurokit')       \n",
    "        # Plot the processed dataframe, normalizing all variables for viewing purpose\n",
    "        data_filtered = data_filtered.append(pd.Series(cleansed, name = i))\n",
    "        \n",
    "    return data_filtered\n",
    "\n",
    "\n",
    "def featuresExtraction(data):\n",
    "    \n",
    "    # input: complete matrix of signals with NaN values\n",
    "    # output: matrix of computed features for each ECG signal\n",
    "    column_names = []\n",
    "    features_total = pd.DataFrame([], columns=column_names, dtype=np.float32)\n",
    "    for (i, sample) in enumerate(tqdm(data[:].iterrows(), total = len(data))):\n",
    "    \n",
    "            \n",
    "        ECG = sample[1].dropna().to_numpy(dtype='float32')\n",
    "        sample_rate = 300\n",
    "\n",
    "\n",
    "        try:\n",
    "            wd, m = hp.process(ECG, 300)\n",
    "            \n",
    "\n",
    "            features = pd.DataFrame()\n",
    "            features = features.append(pd.Series([m[key] for key in m.keys()]), ignore_index=True)\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            features = pd.DataFrame()\n",
    "            features = features.append(pd.Series(np.zeros((13))),ignore_index=True)\n",
    "        \n",
    "\n",
    "        '''\n",
    "        hrv_time\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            peaks1, info = nk.ecg_peaks(ECG, sampling_rate=sample_rate) # 0 or 1 for each position (if 1 then it is RR)\n",
    "            peaks2 = ecg.engzee_segmenter(ECG, 300)['rpeaks']\n",
    "\n",
    "            if (peaks1.ECG_R_Peaks.sum()<5): # main method fails\n",
    "                # 2nd method: detection of the TA from AML\n",
    "                if (peaks2.shape[0]<5): #2nd method fails\n",
    "                    # if 2nd method fails: use third one\n",
    "                    peaks3 = ecg.hamilton_segmenter(signal=ECG, sampling_rate=sample_rate)                \n",
    "                    peaks3 = np.array(peaks3)[0]\n",
    "                    peaks = peaks3\n",
    "                else:\n",
    "                    peaks = peaks2\n",
    "            else:\n",
    "                peaks = peaks1\n",
    "\n",
    "\n",
    "            hrv_time = nk.hrv_time(peaks, sampling_rate=sample_rate, show=False)\n",
    "            hrv_freq = nk.hrv_frequency(peaks, sampling_rate=sample_rate, show=False, normalize=True)\n",
    "            hrv_non = nk.hrv_nonlinear(peaks, sampling_rate=100, show=False)\n",
    "\n",
    "\n",
    "            #hrv_non = nk.hrv_nonlinear(peaks, sampling_rate=300, show=False)\n",
    "\n",
    "        except: #AttributeError:\n",
    "            peaks = ecg.engzee_segmenter(ECG, 300)['rpeaks']\n",
    "            hrv_time = nk.hrv_time(peaks, sampling_rate=sample_rate, show=False)\n",
    "            hrv_freq = nk.hrv_frequency(peaks, sampling_rate=sample_rate, show=False, normalize=True)\n",
    "            hrv_non = nk.hrv_nonlinear(peaks, sampling_rate=100, show=False)\n",
    "\n",
    "        #hrv_time = hrv_time.drop(['HRV_SDANN1', 'HRV_SDNNI1', 'HRV_SDANN2', 'HRV_SDNNI2', 'HRV_SDANN5', 'HRV_SDNNI5'], axis=1)\n",
    "\n",
    "        features = pd.concat([features,hrv_time], axis = 1)      \n",
    "\n",
    "        features = pd.concat([features,hrv_freq], axis = 1)      \n",
    "        features = pd.concat([features,hrv_non], axis = 1)\n",
    "        \n",
    "        \n",
    "        indicesZeroCrossing = nk.signal_zerocrossings(ECG, direction='both')\n",
    "        zeroCross = len(indicesZeroCrossing)/len(ECG)\n",
    "        features['zeroCrossing'] = zeroCross\n",
    "        kurtosisValue = kurtosis(ECG)\n",
    "        features['kurtosis'] = kurtosisValue\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        psdQRS = nk.signal_psd(ECG, sampling_rate=300, show=False, min_frequency=5, max_frequency=25)\n",
    "        powerQRS = psdQRS.sum().loc['Power']\n",
    "        features['powerQRS'] = powerQRS\n",
    "\n",
    "\n",
    "        psd30 = nk.signal_psd(ECG, sampling_rate=300, show=False, min_frequency=30, max_frequency=np.inf)\n",
    "        power30 = psd30.sum().loc['Power']\n",
    "        psdTOTAL = nk.signal_psd(ECG, sampling_rate=300, show=False, min_frequency=0, max_frequency=np.inf)\n",
    "        powerTOTAL = psdTOTAL.sum().loc['Power']\n",
    "        feature2 = power30/powerTOTAL\n",
    "        features['power2'] = feature2\n",
    "        \n",
    "        \n",
    "        # Average beat: take features from it!\n",
    "        if (peaks2.shape[0]<5):\n",
    "            peaks3 = ecg.hamilton_segmenter(signal=ECG, sampling_rate=sample_rate)                \n",
    "            peaks3 = np.array(peaks3)[0]\n",
    "            beats = ecg.extract_heartbeats(ECG, peaks3, 300)['templates']\n",
    "            #snr = computeSNR(ECG, peaks3)\n",
    "            #features['snr'] = snr\n",
    "        else:\n",
    "            beats = ecg.extract_heartbeats(ECG, peaks2, 300)['templates']\n",
    "            #snr = computeSNR(ECG, peaks2)\n",
    "            #features['snr'] = snr\n",
    "\n",
    "        mu = np.mean(beats, axis=0) \n",
    "        var = np.std(beats, axis=0)\n",
    "        md = np.median(beats, axis=0)\n",
    "        maxCorrMuMd = max(correlate(mu, md, mode='full', method='auto')) #feature!!!\n",
    "        features['maxCorrMuMd'] = maxCorrMuMd\n",
    "        varAverageBeat = abs(max(var-mu)) #feature!!!\n",
    "        features['varAverageBeat'] = varAverageBeat\n",
    "        pearson = pearsonr(mu,md)[0] #feature!!!\n",
    "        features['pearson'] = pearson\n",
    "\n",
    "        \n",
    "        # ACF\n",
    "        acf = sm.tsa.acf(np.array(ECG), nlags=ECG.shape[0]-1,fft=False)\n",
    "        acfPeaks = find_peaks(acf, height = 0.1, distance=1)[0]\n",
    "        if (acfPeaks.size==0):\n",
    "            features['stdDiffLocs'] = np.nan\n",
    "        else:\n",
    "            acfLocs = np.append(np.array([1]),acfPeaks)\n",
    "            diffLocs = np.diff(acfLocs)\n",
    "            stdDiffLocs = np.std(diffLocs) #feature!!!\n",
    "            features['stdDiffLocs'] = stdDiffLocs\n",
    "       \n",
    "        try:\n",
    "            quality = nk.ecg_quality(ECG, rpeaks=None, sampling_rate=300, method='zhao2018', approach=None)\n",
    "            quality_out = [\"Unacceptable\", \"Barely Acceptable\", \"Excellent\"]\n",
    "            features['quality'] = quality_out.index(quality)\n",
    "        except Exception as e:\n",
    "            features['quality'] = 0\n",
    "\n",
    "        \n",
    "        features_total = pd.concat([features_total,features], axis = 0)\n",
    "        try:\n",
    "            ECG = nk.ecg_clean(ECG, sampling_rate=sampling_rate, method=\"neurokit\")\n",
    "            _, rpeaks = nk.ecg_peaks(ECG, sampling_rate=300)    \n",
    "            signals, waves = nk.ecg_delineate(ECG, rpeaks, sampling_rate=300)\n",
    "            print(waves)\n",
    "\n",
    "            #Feature: Relative amount of peaks\n",
    "            features['Relative_Amount_r'] = pd.Series((len(rpeaks['ECG_R_Peaks']) / len(ECG)))\n",
    "            features['Relative_Amount_p'] = pd.Series(np.argwhere(np.isnan(waves[\"ECG_P_Peaks\"])).shape[0] / len(ECG))\n",
    "            features['Relative_Amount_t'] = pd.Series(np.argwhere(np.isnan(waves[\"ECG_T_Peaks\"])).shape[0] / len(ECG))\n",
    "            features['Relative_Amount_q'] = pd.Series(np.argwhere(np.isnan(waves[\"ECG_Q_Peaks\"])).shape[0] / len(ECG))\n",
    "            features['Relative_Amount_s'] = pd.Series(np.argwhere(np.isnan(waves[\"ECG_S_Peaks\"])).shape[0] / len(ECG))\n",
    "\n",
    "            features['Amplitude_r'] = pd.Series(np.mean(ECG[rpeaks['ECG_R_Peaks']]))\n",
    "            features['Amplitude_p'] = pd.Series(np.mean(ECG[(np.array(waves['ECG_P_Peaks'])[~np.isnan(np.array(waves['ECG_P_Peaks']))]).astype(int)]))\n",
    "            features['Amplitude_t'] = pd.Series(np.mean(ECG[(np.array(waves['ECG_T_Peaks'])[~np.isnan(np.array(waves['ECG_T_Peaks']))]).astype(int)]))\n",
    "            features['Amplitude_q'] = pd.Series(np.mean(ECG[(np.array(waves['ECG_Q_Peaks'])[~np.isnan(np.array(waves['ECG_Q_Peaks']))]).astype(int)]))\n",
    "            features['Amplitude_s'] = pd.Series(np.mean(ECG[(np.array(waves['ECG_S_Peaks'])[~np.isnan(np.array(waves['ECG_S_Peaks']))]).astype(int)]))\n",
    "            features['Amplitude_r_std'] = pd.Series(np.std(ECG[rpeaks['ECG_R_Peaks']]))\n",
    "            features['Amplitude_p_std'] = pd.Series(np.std(ECG[(np.array(waves['ECG_P_Peaks'])[~np.isnan(np.array(waves['ECG_P_Peaks']))]).astype(int)]))\n",
    "            features['Amplitude_t_std'] = pd.Series(np.std(ECG[(np.array(waves['ECG_T_Peaks'])[~np.isnan(np.array(waves['ECG_T_Peaks']))]).astype(int)]))\n",
    "            features['Amplitude_q_std'] = pd.Series(np.std(ECG[(np.array(waves['ECG_Q_Peaks'])[~np.isnan(np.array(waves['ECG_Q_Peaks']))]).astype(int)]))\n",
    "            features['Amplitude_s_std'] = pd.Series(np.std(ECG[(np.array(waves['ECG_S_Peaks'])[~np.isnan(np.array(waves['ECG_S_Peaks']))]).astype(int)]))\n",
    "            \n",
    "            features_total = features_total.append(features)\n",
    "\n",
    "        except:\n",
    "            features['Relative_Amount_r'] = pd.Series(np.nan)\n",
    "            features['Relative_Amount_p'] = pd.Series(np.nan)\n",
    "            features['Relative_Amount_t'] = pd.Series(np.nan)\n",
    "            features['Relative_Amount_q'] = pd.Series(np.nan)\n",
    "            features['Relative_Amount_s'] = pd.Series(np.nan)\n",
    "\n",
    "            features['Amplitude_r'] = pd.Series(np.nan)\n",
    "            features['Amplitude_p'] = pd.Series(np.nan)\n",
    "            features['Amplitude_t'] = pd.Series(np.nan)\n",
    "            features['Amplitude_q'] = pd.Series(np.nan)\n",
    "            features['Amplitude_s'] = pd.Series(np.nan)\n",
    "\n",
    "            features['Amplitude_r_std'] = pd.Series(np.nan)\n",
    "            features['Amplitude_p_std'] = pd.Series(np.nan)\n",
    "            features['Amplitude_t_std'] = pd.Series(np.nan)\n",
    "            features['Amplitude_q_std'] = pd.Series(np.nan)\n",
    "            features['Amplitude_s_std'] = pd.Series(np.nan)\n",
    "            \n",
    "            features_total = features_total.append(features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return features_total\n",
    "\n",
    "def featuresExtraction_add(data):\n",
    "    \n",
    "    # input: complete matrix of signals with NaN values\n",
    "    # output: matrix of computed features for each ECG signal\n",
    "    column_names = []\n",
    "    features = pd.DataFrame()\n",
    "\n",
    "    features_total = pd.DataFrame([], columns=column_names, dtype=np.float32)\n",
    "    for (i, sample) in enumerate(tqdm(data[:].iterrows(), total = len(data))):\n",
    "    \n",
    "            \n",
    "        ECG = sample[1].dropna().to_numpy(dtype='float32')\n",
    "        sampling_rate = 300\n",
    "\n",
    "        try:\n",
    "\n",
    "            time_r_peaks, amplitude_r_peaks = bsnb.detect_r_peaks(ECG, 300, time_units=True, plot_result= False)\n",
    "\n",
    "            # Finding the maximum and minimum values of the ECG signal\n",
    "            max_ecg = max(ECG)\n",
    "            min_ecg = min(ECG)\n",
    "\n",
    "            # Calculating the amplitude of the signal\n",
    "            vpp_signal_ecg = max_ecg - min_ecg\n",
    "\n",
    "            # Notice that this procedure is condensed in a single function in the numpy Python package:\n",
    "            vpp_signal_ecg = ptp(ECG)\n",
    "            vpp_noise_ecg = []\n",
    "\n",
    "            # For this task, we will follow the same procedure as shown before, but store the values in a list, so that we can then calculate the mean value.\n",
    "            for t in time_r_peaks:\n",
    "                start = int((t + 0.5) * 300) # 0.5 - time between a peak and a flat \n",
    "                end = int((t + 0.65)* 300) # 0.65 time between a peak and the end of the flat\n",
    "                interval = ECG[start:end]\n",
    "                vpp = ptp(interval)\n",
    "                vpp_noise_ecg.append(vpp)\n",
    "                \n",
    "            vpp_noise_ecg = mean(vpp_noise_ecg)\n",
    "\n",
    "            snr_ecg = vpp_signal_ecg/vpp_noise_ecg\n",
    "\n",
    "            # The multiplication by 20 is because the signals are in the unit of (micro)Siemes\n",
    "            snr_ecg_db = 20 * log10(snr_ecg)\n",
    "\n",
    "            features['SNR'] = pd.Series(snr_ecg)\n",
    "\n",
    "            \n",
    "\n",
    "        except:\n",
    "            features['SNR'] = pd.Series(np.NaN)\n",
    "        \n",
    "        \n",
    "        features_total = features_total.append(features)\n",
    "\n",
    "\n",
    "        try:\n",
    "            ECG = nk.ecg_clean(ECG, sampling_rate=sampling_rate, method=\"neurokit\")\n",
    "            _, rpeaks = nk.ecg_peaks(ECG, sampling_rate=300)    \n",
    "            signals, waves = nk.ecg_delineate(ECG, rpeaks, sampling_rate=300)\n",
    "\n",
    "            #Feature: Relative amount of peaks\n",
    "            features['r_diff_mean'] = pd.Series(np.mean(np.diff(rpeaks)))\n",
    "            features['r_diff_std'] = pd.Series(np.std(np.diff(rpeaks)))\n",
    "            features['r_ampl_std'] = pd.Series(np.std(ECG[rpeaks['ECG_R_Peaks']]))\n",
    "            \n",
    "\n",
    "            features['p_diff_mean'] = pd.Series(np.mean(np.diff(waves[\"ECG_P_Peaks\"][np.argwhere(np.isnan(waves[\"ECG_P_Peaks\"]))])))\n",
    "            features['p_diff_mean'] = pd.Series(np.std(np.diff(waves[\"ECG_P_Peaks\"][np.argwhere(np.isnan(waves[\"ECG_P_Peaks\"]))])))\n",
    "            features['p_ampl_std'] = pd.Series(np.std(ECG[(np.array(waves['ECG_P_Peaks'])[~np.isnan(np.array(waves['ECG_P_Peaks']))]).astype(int)]))\n",
    "\n",
    "            features['t_diff_mean'] = pd.Series(np.mean(np.diff(waves[\"ECG_T_Peaks\"][np.argwhere(np.isnan(waves[\"ECG_T_Peaks\"]))])))\n",
    "            features['t_diff_mean'] = pd.Series(np.std(np.diff(waves[\"ECG_T_Peaks\"][np.argwhere(np.isnan(waves[\"ECG_T_Peaks\"]))])))\n",
    "            features['t_ampl_std'] = pd.Series(np.std(ECG[(np.array(waves['ECG_T_Peaks'])[~np.isnan(np.array(waves['ECG_T_Peaks']))]).astype(int)]))\n",
    "\n",
    "            features['q_diff_mean'] = pd.Series(np.mean(np.diff(waves[\"ECG_Q_Peaks\"][np.argwhere(np.isnan(waves[\"ECG_Q_Peaks\"]))])))\n",
    "            features['q_diff_mean'] = pd.Series(np.std(np.diff(waves[\"ECG_Q_Peaks\"][np.argwhere(np.isnan(waves[\"ECG_Q_Peaks\"]))])))\n",
    "            features['q_ampl_std'] = pd.Series(np.std(ECG[(np.array(waves['ECG_Q_Peaks'])[~np.isnan(np.array(waves['ECG_Q_Peaks']))]).astype(int)]))\n",
    "\n",
    "\n",
    "            features['s_diff_mean'] = pd.Series(np.mean(np.diff(waves[\"ECG_S_Peaks\"][np.argwhere(np.isnan(waves[\"ECG_S_Peaks\"]))])))\n",
    "            features['s_diff_mean'] = pd.Series(np.std(np.diff(waves[\"ECG_S_Peaks\"][np.argwhere(np.isnan(waves[\"ECG_S_Peaks\"]))])))\n",
    "            features['s_ampl_std'] = pd.Series(np.std(ECG[(np.array(waves['ECG_S_Peaks'])[~np.isnan(np.array(waves['ECG_S_Peaks']))]).astype(int)]))\n",
    "            \n",
    "            features_total = features_total.append(features)\n",
    "\n",
    "        except:\n",
    "            features['r_diff_mean'] = pd.Series(np.nan)\n",
    "            features['r_diff_std'] = pd.Series(np.nan)\n",
    "            features['r_ampl_std'] = pd.Series(np.nan)\n",
    "\n",
    "            features['p_diff_mean'] = pd.Series(np.nan)\n",
    "            features['p_diff_mean'] = pd.Series(np.nan)\n",
    "            features['p_ampl_std'] = pd.Series(np.nan)\n",
    "\n",
    "            features['t_diff_mean'] = pd.Series(np.nan)\n",
    "            features['t_diff_mean'] = pd.Series(np.nan)\n",
    "            features['t_ampl_std'] = pd.Series(np.nan)\n",
    "\n",
    "            features['q_diff_mean'] = pd.Series(np.nan)\n",
    "            features['q_diff_mean'] = pd.Series(np.nan)\n",
    "            features['q_ampl_std'] = pd.Series(np.nan)\n",
    "\n",
    "            features['s_diff_mean'] = pd.Series(np.nan)\n",
    "            features['s_diff_mean'] = pd.Series(np.nan)\n",
    "            features['s_ampl_std'] = pd.Series(np.nan)\n",
    "            \n",
    "            features_total = features_total.append(features)\n",
    "            print(features)\n",
    "            break\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return features_total    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fillingNaN(features, option):\n",
    "    # fillingNaN:\n",
    "    # 1.- iterative imputer\n",
    "    # 2.- nan replaced by median + 3xstd\n",
    "\n",
    "    if (option==1):\n",
    "        imp = IterativeImputer(max_iter=10, random_state=1, n_nearest_features = 28, verbose = 0)\n",
    "        featuresFinal = imp.fit_transform(np.array(features))\n",
    "        \n",
    "    else:\n",
    "        #np.where(np.isnan(features), ma.array(features, mask=np.isnan(features)).np.nanmean(axis=0), features)    \n",
    "        col_med = np.nanmedian(features, axis=0)\n",
    "        col_std = np.nanstd(features, axis=0)\n",
    "        replace = col_med + col_std\n",
    "        featuresFinal= np.where(np.isnan(features),replace,features)\n",
    "        #inds = np.where(np.isnan(features))\n",
    "        #features[inds] = np.take(col_med+3*col_std, inds[1])\n",
    "\n",
    "    return featuresFinal\n",
    "\n",
    "def scaler(X_train):\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    return X_train\n",
    "\n",
    "\n",
    "def feature_selection(x,y,k):\n",
    "    selector = SelectKBest(f_classif, k=k)\n",
    "    selector.fit(x, y)\n",
    "    cols = selector.get_support(indices=True)\n",
    "    return pd.DataFrame(x).iloc[:,cols], cols\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3a8f5",
   "metadata": {},
   "source": [
    "### Import data and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f059ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "'''\n",
    "if not('x_train' in locals()):\n",
    "    x_train = loadData('X_train.csv')\n",
    "if not('x_test' in locals()):\n",
    "    x_test = loadData('X_test.csv')\n",
    "'''    \n",
    "if not('y_train' in locals()):\n",
    "    y_train = loadData('y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6470d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "#clean data\n",
    "if not('x_train_p' in locals()):\n",
    "    if os.path.isfile('X_train_clean.csv'):\n",
    "        x_train_p = pd.read_csv('X_train_clean.csv')\n",
    "    else: \n",
    "        x_train_p = processSignal(x_train)\n",
    "        x_train_p.to_csv('X_train_clean.csv')\n",
    "\n",
    "if not('x_test_p' in locals()):\n",
    "    if os.path.isfile('X_test_clean.csv'):\n",
    "        x_test_p = pd.read_csv('X_test_clean.csv')\n",
    "    else: \n",
    "        x_test_p = processSignal(x_test)\n",
    "        x_test_p.to_csv('X_test_clean.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b42c3c8",
   "metadata": {},
   "source": [
    "### Extraction of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features\n",
    "#if not('x_train_f' in locals()):\n",
    "if os.path.isfile('X_train_features_add.csv'):\n",
    "    x_train_f_add = pd.read_csv('X_train_features_add.csv')\n",
    "else: \n",
    "    x_train_f_add = featuresExtraction_add(x_train_clean)\n",
    "    x_train_f_add.to_csv('X_train_features_add.csv')\n",
    "\n",
    "#if not('x_test_f' in locals()):\n",
    "if os.path.isfile('X_test_features_add.csv'):\n",
    "    x_test_f_add = pd.read_csv('X_test_features_add.csv')\n",
    "else: \n",
    "    x_test_f_add = featuresExtraction_add(x_test_clean)\n",
    "    x_test_f_add.to_csv('X_test_features_add.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0c736",
   "metadata": {},
   "source": [
    "### Dealing with NaN columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: drop columns with NAN\n",
    "x_train_final = x_train_f.dropna(axis = 1,how = 'all')\n",
    "x_test_final = x_test_f.dropna(axis = 1,how = 'all')\n",
    "#x_train_final = pd.read_csv('X_train_features_unclean.csv').dropna(axis = 1,how = 'all')\n",
    "#x_test_final = pd.read_csv('X_test_features_unclean.csv').dropna(axis = 1,how = 'all')\n",
    "x_train_2 = pd.read_csv('features_matlab/feats_X_train_Prep.csv', header = None).dropna(axis = 1,how = 'all')\n",
    "x_test_2 = pd.read_csv('features_matlab/feats_X_test_Prep.csv', header = None).dropna(axis = 1,how = 'all')\n",
    "\n",
    "x_train_add1 = pd.read_csv('features_add/X_train_features_add1.csv').dropna(axis = 1,how = 'all')\n",
    "x_test_add1 = pd.read_csv('features_add/X_test_features_add1.csv').dropna(axis = 1,how = 'all')\n",
    "\n",
    "x_train_add2 = pd.read_csv('features_add/X_train_features_add.csv').dropna(axis = 1,how = 'all')\n",
    "x_test_add2 = pd.read_csv('features_add/X_test_features_add.csv').dropna(axis = 1,how = 'all')\n",
    "\n",
    "x_train_add3 = pd.read_csv('features_add/X_test_features_add3.csv').dropna(axis = 1,how = 'all')\n",
    "x_test_add3 = pd.read_csv('features_add/X_test_features_add3.csv').dropna(axis = 1,how = 'all')\n",
    "\n",
    "\n",
    "x_train_final = pd.concat([pd.DataFrame(x_train_final).reset_index(), \\\n",
    "                           pd.DataFrame(x_train_2), \\\n",
    "                           #pd.Series(x_train_f_add['SNR']).reset_index(), \\\n",
    "                           pd.DataFrame(x_train_add1),\n",
    "                           pd.DataFrame(x_train_add2),\n",
    "                           pd.DataFrame(x_train_add3)], axis = 1)\n",
    "\n",
    "x_test_final = pd.concat([pd.DataFrame(x_test_final).reset_index(), \\\n",
    "                          pd.DataFrame(x_test_2), \\\n",
    "                          #pd.Series(x_test_f_add['SNR']).reset_index(), \\\n",
    "                          pd.DataFrame(x_test_add1),\n",
    "                          pd.DataFrame(x_test_add2),\n",
    "                          pd.DataFrame(x_test_add3)], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "#print([type for type in x_test_final.dtypes])\n",
    "#print(x_train_final[x_train_final.columns[5]])\n",
    "\n",
    "# Second: filling missing values (I chose 2 methods: iterative imputer or with median + 3·std)\n",
    "x_train_final = x_train_final.drop(columns = ['3'])\n",
    "x_test_final = x_test_final.drop(columns = ['3'])\n",
    "#x_train_final = x_train_final.drop(columns = ['quality'])\n",
    "\n",
    "\n",
    "x_train_final = x_train_final.astype(float)\n",
    "x_test_final = x_test_final.astype(float)\n",
    "\n",
    "x_train_final = x_train_final.replace([np.inf, -np.inf], np.nan)\n",
    "x_test_final = x_test_final.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "x_train_final = fillingNaN(x_train_final, option=1)\n",
    "x_test_final = fillingNaN(x_test_final, option=1)\n",
    "print(pd.DataFrame(x_train_final))\n",
    "#feature selection\n",
    "x_train_final, cols = feature_selection(x_train_final, y_train,80)\n",
    "x_test_final = pd.DataFrame(x_test_final).iloc[:,cols]\n",
    "\n",
    "# Third: scaler\n",
    "x_train_final = scaler(x_train_final)\n",
    "x_test_final = scaler(x_test_final)\n",
    "\n",
    "#rest against 3\n",
    "y_train_1_3 = np.where(y_train['y'] < 3, 0,1)\n",
    "\n",
    "#01 against 2\n",
    "y_train_1_2 = y_train[y_train['y'] != 3]\n",
    "\n",
    "x_train_1_2 = pd.DataFrame(x_train_final).loc[y_train_1_2.index]\n",
    "\n",
    "\n",
    "#0 against 1\n",
    "#y_train_0_1 = pd.DataFrame(y_train_1_2[y_train_1_2 < 1])\n",
    "#x_train_0_1 = pd.DataFrame(x_train_1_2).loc[y_train_0_1.index]\n",
    "print(x_train_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84b26a",
   "metadata": {},
   "source": [
    "### Classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b9605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "#classification\n",
    "#clf_1_3 = GradientBoostingClassifier(random_state=0, n_estimators=300, max_depth = 4)\n",
    "#clf_1_2 = GradientBoostingClassifier(random_state=0, n_estimators=300, max_depth = 4)\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=0, n_estimators=100, max_depth = 4)\n",
    "\n",
    "#clf_1_3 = GradientBoostingClassifier(random_state=0, n_estimators=100, max_depth = 3, learning_rate =0.01)\n",
    "#clf_0_1 = GradientBoostingClassifier(random_state=0, n_estimators=100, max_depth = 3, learning_rate =0.01)\n",
    "\n",
    "#clf = GaussianNB()\n",
    "#clf = MLPClassifier(hidden_layer_sizes=(128,256,500,560,256,128), early_stopping=True, max_iter=1000)\n",
    "#clf = RUSBoostClassifier(n_estimators=500)\n",
    "#scores1 = cross_val_score(clf_1_3, x_train_final, y_train_1_3, cv=5, scoring='f1_micro')\n",
    "#scores2 = cross_val_score(clf_1_2, x_train_1_2, y_train_1_2, cv=5, scoring='f1_micro')\n",
    "scores = cross_val_score(clf, x_train_final, y_train, cv=5, scoring='f1_micro')\n",
    "\n",
    "#scores1 = cross_val_score(clf_1_3, x_train_final, y_train_1_3, cv=5, scoring='f1_micro')\n",
    "#scores2 = cross_val_score(clf_1_2, x_train_1_2, y_train_1_2, cv=5, scoring='f1_micro')\n",
    "#scores3 = cross_val_score(clf_0_1, x_train_0_1, y_train_0_1, cv=5, scoring='f1_micro')\n",
    "\n",
    "\n",
    "print(scores)\n",
    "#print(scores1,scores2, scores3)\n",
    "print('mean score', np.mean(scores))\n",
    "\n",
    "pygame.mixer.init()\n",
    "pygame.mixer.music.load(sound_file)\n",
    "pygame.mixer.music.play()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1_3.fit(x_train_final, y_train_1_3)\n",
    "clf_1_2.fit(x_train_1_2, y_train_1_2)\n",
    "#clf_0_1.fit(x_train_0_1, y_train_0_1)\n",
    "\n",
    "#clf.fit(x_train_final, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79511128",
   "metadata": {},
   "source": [
    "### Prediction in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1_3 = clf_1_3.predict(x_test_final)\n",
    "x_test_1_2 = pd.DataFrame(x_test_final[np.where(pred_1_3 == 0)])\n",
    "print(x_test_1_2.shape)\n",
    "\n",
    "pred_1_2 = clf_1_2.predict(x_test_1_2)\n",
    "\n",
    "\n",
    "\n",
    "mask_pred = pred_1_3 == 1\n",
    "\n",
    "\n",
    "#pred_1_2 = clf_1_2.predict(x_test_final)\n",
    "#pred_0_1 = clf_0_1.predict(x_test_final)\n",
    "\n",
    "#mask_pred = pred_1_3 == 1\n",
    "#pred_0_1_2 = clf_0_1_2.predict(x_test_final[mask_pred])\n",
    "#pred = clf.predict(x_test_final)\n",
    "pred_test = pd.DataFrame()\n",
    "pred_test['id'] = np.arange(len(pred_1_3))\n",
    "pred_test['y'] = pred_1_3+2\n",
    "print(len(pred_test['y'][y_train['y'] != 3]), len(pred_1_2))\n",
    "pred_test['y'][pred_test['y'] == 2] = pred_1_2\n",
    "#pred_test['y'] = pred\n",
    "pred_test.to_csv('results.csv' ,index=False)\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da4f85",
   "metadata": {},
   "source": [
    "### Checking the amount of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3689d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(pred_test['y']))\n",
    "pred_test.y.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "722a6f6468c40eb10e080dc076ad1be2feff99a7f31db43232e41a319206c6e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
